{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lib2to3.pgen2 import token\n",
    "from tracemalloc import start\n",
    "import numpy as np\n",
    "import csv\n",
    "import keras\n",
    "from sklearn.utils import shuffle \n",
    "import time as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "device = 'cuda'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def runtime(starttime):\n",
    "    print(\"Runtime: {}\".format(tt.time() - starttime))\n",
    "    return 0\n",
    "\n",
    "def unison_shuffle(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def load_pretrained_embeddings(filepath):\n",
    "    print(\"Loading in pretrained GloVe embeddings from: {}\".format(filepath))\n",
    "    \n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as embedfile:\n",
    "        for line in embedfile:\n",
    "            split_line = line.split()\n",
    "            token = split_line[0]\n",
    "            embed = np.array(split_line[1:], dtype=np.float64)\n",
    "            embeddings[token] = embed\n",
    "    \n",
    "    print(\"{} words loaded into embedding\".format(len(embeddings)))\n",
    "    \n",
    "    return (embeddings)\n",
    "\n",
    "def tokenize_csv(filename):\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "\n",
    "    with open(filename, mode='r',encoding='utf-8') as csvfile:\n",
    "        tweetreader = csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for sentence, label in tweetreader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(sentence.encode('ascii', 'ignore').decode('ascii')))\n",
    "                if label=='sadness':\n",
    "                    lab.append([0,0,0,0,1])\n",
    "                elif label=='happiness':\n",
    "                    lab.append([0,0,0,1,0])\n",
    "                elif label=='no emotion':\n",
    "                    lab.append([0,0,1,0,0])\n",
    "                elif label=='fear':\n",
    "                    lab.append([0,1,0,0,0])\n",
    "                else:\n",
    "                    lab.append([1,0,0,0,0])\n",
    "\n",
    "    return (sen, lab)\n",
    "\n",
    "def tokenize_manual(filename, label):\n",
    "    '''\n",
    "        00001 sadness \n",
    "        00010 happiness\n",
    "        00100 no emotion\n",
    "        01000 fear\n",
    "        10000 anger\n",
    "    '''\n",
    "\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "    with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "        treader=csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for i_d, t in treader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(t.encode('ascii', 'ignore').decode('ascii')))\n",
    "                lab.append(label)\n",
    "    \n",
    "    return (sen, lab)\n",
    "\n",
    "def tweet_vectorize(tweet_label_in, glove_embeds):\n",
    "    out_vectors = []\n",
    "\n",
    "    tweet_list = tweet_label_in[0]\n",
    "    label_list = tweet_label_in[1]\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        curr_tweet = []\n",
    "        while len(tweet_list[i]) > 0 and len(curr_tweet) < 50:\n",
    "            curr_word = tweet_list[i].pop(0)\n",
    "            if curr_word in glove_embeds:\n",
    "                curr_tweet.append(glove_embeds[curr_word])\n",
    "            else:\n",
    "                curr_tweet.append(np.zeros(50))     # dimensionality of embedding vector\n",
    "        if len(curr_tweet) < 50:\n",
    "            while len(curr_tweet) < 50:\n",
    "                curr_tweet.append(np.zeros(50))\n",
    "        out_vectors.append(curr_tweet)\n",
    "    \n",
    "    return (out_vectors, label_list)\n",
    "\n",
    "def split_train_val_test(input, labels, man_input, man_lab):\n",
    "\n",
    "  input_shuff, labels_shuff = shuffle(input, labels)\n",
    "\n",
    "  training_proportion = 0.8\n",
    "  validation_proportion = 0.1\n",
    "  num_train = int(len(input_shuff) * training_proportion)\n",
    "  num_val = int(len(input_shuff) * validation_proportion)\n",
    "\n",
    "  input_train, input_valid, input_test = input_shuff[:num_train], input_shuff[num_train:num_train+num_val], input_shuff[num_train+num_val:]\n",
    "  label_train, label_valid, label_test = labels_shuff[:num_train], labels_shuff[num_train:num_train+num_val], labels_shuff[num_train+num_val:]\n",
    "\n",
    "  input_test += man_input\n",
    "  label_test += man_lab\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "def convert_to_tensors(input_train, input_valid, input_test, label_train, label_valid, label_test):\n",
    "  input_train = torch.as_tensor(input_train)\n",
    "  input_valid = torch.as_tensor(input_valid)\n",
    "  input_test = torch.as_tensor(input_test)\n",
    "  label_train = torch.as_tensor(label_train)\n",
    "  label_valid = torch.as_tensor(label_valid)\n",
    "  label_test = torch.as_tensor(label_test)\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "# Model Definition\n",
    "class model(nn.Module):\n",
    "    def __init__(self, in_channels=50, out_channels=30, kernel_size=3, stride=1, hidden_dim=4, num_layers=4):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convolution_layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride),      # IN: [batch_size, num_vecs=50, embed_dim=50], OUT: [batch_size, out_channel=30, convolved=48]\n",
    "            nn.BatchNorm1d(out_channels),                                   # unchanged; in-place op\n",
    "            nn.ReLU(),                                                      # unchanged; in-place ops\n",
    "            nn.MaxPool1d(kernel_size))                                      # IN: [batch_size, out_channel=30, convolved=48], OUT: [batch_size, 30, floor(convolved/kernel_size)=16]\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=int((self.in_channels - self.kernel_size + 1)/self.kernel_size), hidden_size=hidden_dim, bidirectional=True)   # IN: [batch_size, 30, 16], OUT: [batch_size, 30, 8]\n",
    "\n",
    "        self.lin1 = nn.Linear(30, 1)\n",
    "        self.lin2 = nn.Linear(8, 5)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.convolution_layer1(x)\n",
    "        x, (hn, cn) = self.bilstm(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.lin2(x)\n",
    "        x = self.softmax(x)\n",
    "        return(x)\n",
    "\n",
    "# training loop\n",
    "def training_loop(model_instance, loss, optimizer, epochs, batch_size, x_train, y_train):\n",
    "  for epoch in range(epochs):\n",
    "    prediction = model_instance(x_train)\n",
    "    print(prediction.shape)\n",
    "    loss_value = loss(prediction, y_train)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in pretrained GloVe embeddings from: D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\n",
      "400000 words loaded into embedding\n"
     ]
    }
   ],
   "source": [
    "runstart = tt.time()\n",
    "small_embeds = load_pretrained_embeddings(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished vectorizing csv number 1 Dataset size:  28091 28091\n"
     ]
    }
   ],
   "source": [
    "training_set = [[], []]\n",
    "for i in range(1, 2):\n",
    "    train_tokens = tokenize_csv(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/tweets_labels/data{}.csv\".format(str(i)))\n",
    "    temp_vec = tweet_vectorize(train_tokens, small_embeds)\n",
    "    training_set[0] = training_set[0] + temp_vec[0]\n",
    "    training_set[1] = training_set[1] + temp_vec[1]\n",
    "    print('Finished vectorizing csv number {}'.format(str(i)), \"Dataset size: \", len(training_set[0]), len(training_set[1]))\n",
    "\n",
    "manual_names = ['anger', 'fear', 'happy', 'sad']\n",
    "manual_labels = [[1,0,0,0,0], [0,1,0,0,0], [0,0,0,1,0], [0,0,0,0,1]]\n",
    "\n",
    "manual_set = [[], []]\n",
    "for i in range(len(manual_labels)):\n",
    "    manual_tokens = tokenize_manual(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/manually_labeled/data_{}.csv\".format(manual_names[i]), manual_labels[i])\n",
    "    temp_vec = tweet_vectorize(manual_tokens, small_embeds)\n",
    "    manual_set[0] = manual_set[0] + temp_vec[0]\n",
    "    manual_set[1] = manual_set[1] + temp_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test = split_train_val_test(training_set[0], training_set[1], manual_set[0], manual_set[1])\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = convert_to_tensors(x_train, x_valid, x_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22472, 5])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = torch.squeeze(y_train, dim=1)\n",
    "y_valid = torch.squeeze(y_valid, dim=1)\n",
    "y_test = torch.squeeze(y_test, dim=1)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = model()\n",
    "loss = nn.BCELoss()         # need a loss function\n",
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr=0.01)      # need an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyx\\AppData\\Local\\Temp/ipykernel_21612/955908134.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21612/1063430589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "# training_loop(model_instance, loss, optimizer, 10, 30, x_train, y_train)\n",
    "\n",
    "for epoch in range(30):\n",
    "    prediction = model_instance(x_train)\n",
    "    prediction = torch.squeeze(prediction, dim=1)\n",
    "    # prediction = nn.functional.one_hot(prediction, 5)\n",
    "    print(prediction.shape)\n",
    "    loss_value = loss(prediction, y_train)\n",
    "    print(loss_value)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10e1abf77ddef6b5a0d462bd0c1af8a6d69cacf984b7ff54f171d3af9bd75ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
