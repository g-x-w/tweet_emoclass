{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lib2to3.pgen2 import token\n",
    "from tracemalloc import start\n",
    "import numpy as np\n",
    "import csv\n",
    "import keras\n",
    "from sklearn.utils import shuffle \n",
    "# import torch\n",
    "import time as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "device = 'cuda'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def runtime(starttime):\n",
    "    print(\"Runtime: {}\".format(tt.time() - starttime))\n",
    "    return 0\n",
    "\n",
    "def unison_shuffle(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def load_pretrained_embeddings(filepath):\n",
    "    print(\"Loading in pretrained GloVe embeddings from: {}\".format(filepath))\n",
    "    \n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as embedfile:\n",
    "        for line in embedfile:\n",
    "            split_line = line.split()\n",
    "            token = split_line[0]\n",
    "            embed = np.array(split_line[1:], dtype=np.float64)\n",
    "            embeddings[token] = embed\n",
    "    \n",
    "    print(\"{} words loaded into embedding\".format(len(embeddings)))\n",
    "    \n",
    "    return (embeddings)\n",
    "\n",
    "def tokenize_csv(filename):\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "\n",
    "    with open(filename, mode='r',encoding='utf-8') as csvfile:\n",
    "        tweetreader = csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for sentence, label in tweetreader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(sentence.encode('ascii', 'ignore').decode('ascii')))\n",
    "                if label=='sadness':\n",
    "                    lab.append([0,0,0,0,1])\n",
    "                elif label=='happiness':\n",
    "                    lab.append([0,0,0,1,0])\n",
    "                elif label=='no emotion':\n",
    "                    lab.append([0,0,1,0,0])\n",
    "                elif label=='fear':\n",
    "                    lab.append([0,1,0,0,0])\n",
    "                else:\n",
    "                    lab.append([1,0,0,0,0])\n",
    "\n",
    "    return (sen, lab)\n",
    "\n",
    "def tokenize_manual(filename, label):\n",
    "    '''\n",
    "        00001 sadness \n",
    "        00010 happiness\n",
    "        00100 no emotion\n",
    "        01000 fear\n",
    "        10000 anger\n",
    "    '''\n",
    "\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "    with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "        treader=csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for i_d, t in treader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(t.encode('ascii', 'ignore').decode('ascii')))\n",
    "                lab.append(label)\n",
    "    \n",
    "    return (sen, lab)\n",
    "\n",
    "def tweet_vectorize(tweet_label_in, glove_embeds):\n",
    "    out_vectors = []\n",
    "\n",
    "    tweet_list = tweet_label_in[0]\n",
    "    label_list = tweet_label_in[1]\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        curr_tweet = []\n",
    "        while len(tweet_list[i]) > 0 and len(curr_tweet) < 50:\n",
    "            curr_word = tweet_list[i].pop(0)\n",
    "            if curr_word in glove_embeds:\n",
    "                curr_tweet.append(glove_embeds[curr_word])\n",
    "            else:\n",
    "                curr_tweet.append(np.zeros(50))     # dimensionality of embedding vector\n",
    "        if len(curr_tweet) < 50:\n",
    "            while len(curr_tweet) < 50:\n",
    "                curr_tweet.append(np.zeros(50))\n",
    "        out_vectors.append(curr_tweet)\n",
    "    \n",
    "    return (out_vectors, label_list)\n",
    "\n",
    "def split_train_val_test(input, labels, man_input, man_lab):\n",
    "\n",
    "  input_shuff, labels_shuff = shuffle(input, labels)\n",
    "\n",
    "  training_proportion = 0.8\n",
    "  validation_proportion = 0.1\n",
    "  num_train = int(len(input_shuff) * training_proportion)\n",
    "  num_val = int(len(input_shuff) * validation_proportion)\n",
    "\n",
    "  input_train, input_valid, input_test = input_shuff[:num_train], input_shuff[num_train:num_train+num_val], input_shuff[num_train+num_val:]\n",
    "  label_train, label_valid, label_test = labels_shuff[:num_train], labels_shuff[num_train:num_train+num_val], labels_shuff[num_train+num_val:]\n",
    "\n",
    "  input_test += man_input\n",
    "  label_test += man_lab\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "def convert_to_tensors(input_train, input_valid, input_test, label_train, label_valid, label_test):\n",
    "  input_train = torch.as_tensor(input_train)\n",
    "  input_valid = torch.as_tensor(input_valid)\n",
    "  input_test = torch.as_tensor(input_test)\n",
    "  label_train = torch.as_tensor(label_train)\n",
    "  label_valid = torch.as_tensor(label_valid)\n",
    "  label_test = torch.as_tensor(label_test)\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "# Model Definition\n",
    "class model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(model, self).__init__()\n",
    "\n",
    "    self.convolution_layer = nn.Conv1d(in_channels=50, out_channels=30, kernel_size=2, stride=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    x = self.convolution_layer(x)\n",
    "    x = nn.functional.relu(x)\n",
    "    x = nn.functional.max_pool1d(input=x, kernel_size=self.convolution_layer.kernel_size)\n",
    "    return(x)\n",
    "\n",
    "# training loop\n",
    "def training_loop(model_instance, loss, optimizer, epochs, batch_size, x_train, y_train):\n",
    "  for epoch in range(epochs):\n",
    "    prediction = model_instance(x_train)\n",
    "    loss_value = loss(prediction, y_train)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # for batch in range(0, len(x_train), batch_size):\n",
    "    #   current_batch = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in pretrained GloVe embeddings from: D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\n",
      "400000 words loaded into embedding\n"
     ]
    }
   ],
   "source": [
    "runstart = tt.time()\n",
    "small_embeds = load_pretrained_embeddings(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished vectorizing csv number 1 Dataset size:  28091 28091\n"
     ]
    }
   ],
   "source": [
    "training_set = [[], []]\n",
    "for i in range(1, 2):\n",
    "    train_tokens = tokenize_csv(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/tweets_labels/data{}.csv\".format(str(i)))\n",
    "    temp_vec = tweet_vectorize(train_tokens, small_embeds)\n",
    "    training_set[0] = training_set[0] + temp_vec[0]\n",
    "    training_set[1] = training_set[1] + temp_vec[1]\n",
    "    print('Finished vectorizing csv number {}'.format(str(i)), \"Dataset size: \", len(training_set[0]), len(training_set[1]))\n",
    "\n",
    "manual_names = ['anger', 'fear', 'happy', 'sad']\n",
    "manual_labels = [[1,0,0,0,0], [0,1,0,0,0], [0,0,0,1,0], [0,0,0,0,1]]\n",
    "\n",
    "manual_set = [[], []]\n",
    "for i in range(len(manual_labels)):\n",
    "    manual_tokens = tokenize_manual(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/manually_labeled/data_{}.csv\".format(manual_names[i]), manual_labels[i])\n",
    "    temp_vec = tweet_vectorize(manual_tokens, small_embeds)\n",
    "    manual_set[0] = manual_set[0] + temp_vec[0]\n",
    "    manual_set[1] = manual_set[1] + temp_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test = split_train_val_test(training_set[0], training_set[1], manual_set[0], manual_set[1])\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = convert_to_tensors(x_train, x_valid, x_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.unsqueeze(y_train, dim=1)\n",
    "y_valid = torch.unsqueeze(y_valid, dim=1)\n",
    "y_test = torch.unsqueeze(y_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = model()\n",
    "loss = nn.BCELoss()         # need a loss function\n",
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr=0.01)      # need an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyx\\AppData\\Local\\Temp/ipykernel_18124/2601970540.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([22472, 5])) that is different to the input size (torch.Size([22472, 30, 24])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18124/2907318511.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18124/2601970540.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(model_instance, loss, optimizer, epochs, batch_size, x_train, y_train)\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2904\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m             \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([22472, 5])) that is different to the input size (torch.Size([22472, 30, 24])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "training_loop(model_instance, loss, optimizer, 10, 30, x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10e1abf77ddef6b5a0d462bd0c1af8a6d69cacf984b7ff54f171d3af9bd75ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
