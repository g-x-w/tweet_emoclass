{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lib2to3.pgen2 import token\n",
    "from tracemalloc import start\n",
    "import numpy as np\n",
    "import csv\n",
    "import keras\n",
    "from sklearn.utils import shuffle \n",
    "import time as tt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "device = 'cuda'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def runtime(starttime):\n",
    "    ret = tt.time() - starttime\n",
    "    return ret\n",
    "\n",
    "def unison_shuffle(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def load_pretrained_embeddings(filepath):\n",
    "    print(\"Loading in pretrained GloVe embeddings from: {}\".format(filepath))\n",
    "    \n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as embedfile:\n",
    "        for line in embedfile:\n",
    "            split_line = line.split()\n",
    "            token = split_line[0]\n",
    "            embed = np.array(split_line[1:], dtype=np.float64)\n",
    "            embeddings[token] = embed\n",
    "    \n",
    "    print(\"{} words loaded into embedding\".format(len(embeddings)))\n",
    "    \n",
    "    return (embeddings)\n",
    "\n",
    "def tokenize_csv(filename):\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "\n",
    "    with open(filename, mode='r',encoding='utf-8') as csvfile:\n",
    "        tweetreader = csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for sentence, label in tweetreader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(sentence.encode('ascii', 'ignore').decode('ascii')))\n",
    "                if label=='sadness':\n",
    "                    lab.append([0,0,0,0,1])\n",
    "                elif label=='happiness':\n",
    "                    lab.append([0,0,0,1,0])\n",
    "                elif label=='no emotion':\n",
    "                    lab.append([0,0,1,0,0])\n",
    "                elif label=='fear':\n",
    "                    lab.append([0,1,0,0,0])\n",
    "                else:\n",
    "                    lab.append([1,0,0,0,0])\n",
    "\n",
    "    return (sen, lab)\n",
    "\n",
    "def tokenize_manual(filename, label):\n",
    "    '''\n",
    "        00001 sadness \n",
    "        00010 happiness\n",
    "        00100 no emotion\n",
    "        01000 fear\n",
    "        10000 anger\n",
    "    '''\n",
    "\n",
    "    sen=[]\n",
    "    lab=[]\n",
    "    with open(filename, mode='r', encoding='utf-8') as csvfile:\n",
    "        treader=csv.reader(csvfile)\n",
    "        firstline=True\n",
    "        for i_d, t in treader:\n",
    "            if firstline:\n",
    "                firstline=False\n",
    "                continue\n",
    "            else:\n",
    "                sen.append(keras.preprocessing.text.text_to_word_sequence(t.encode('ascii', 'ignore').decode('ascii')))\n",
    "                lab.append(label)\n",
    "    \n",
    "    return (sen, lab)\n",
    "\n",
    "def tweet_vectorize(tweet_label_in, glove_embeds):\n",
    "    out_vectors = []\n",
    "\n",
    "    tweet_list = tweet_label_in[0]\n",
    "    label_list = tweet_label_in[1]\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        curr_tweet = []\n",
    "        while len(tweet_list[i]) > 0 and len(curr_tweet) < 50:\n",
    "            curr_word = tweet_list[i].pop(0)\n",
    "            if curr_word in glove_embeds:\n",
    "                curr_tweet.append(glove_embeds[curr_word])\n",
    "            else:\n",
    "                curr_tweet.append(np.zeros(50))     # dimensionality of embedding vector\n",
    "        if len(curr_tweet) < 50:\n",
    "            while len(curr_tweet) < 50:\n",
    "                curr_tweet.append(np.zeros(50))\n",
    "        out_vectors.append(curr_tweet)\n",
    "    \n",
    "    return (out_vectors, label_list)\n",
    "\n",
    "def split_train_val_test(input, labels, man_input, man_lab):\n",
    "\n",
    "  input_shuff, labels_shuff = shuffle(input, labels)\n",
    "\n",
    "  training_proportion = 0.8\n",
    "  validation_proportion = 0.1\n",
    "  num_train = int(len(input_shuff) * training_proportion)\n",
    "  num_val = int(len(input_shuff) * validation_proportion)\n",
    "\n",
    "  input_train, input_valid, input_test = input_shuff[:num_train], input_shuff[num_train:num_train+num_val], input_shuff[num_train+num_val:]\n",
    "  label_train, label_valid, label_test = labels_shuff[:num_train], labels_shuff[num_train:num_train+num_val], labels_shuff[num_train+num_val:]\n",
    "\n",
    "  input_test += man_input\n",
    "  label_test += man_lab\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "def convert_to_tensors(input_train, input_valid, input_test, label_train, label_valid, label_test):\n",
    "  input_train = torch.as_tensor(input_train)\n",
    "  input_valid = torch.as_tensor(input_valid)\n",
    "  input_test = torch.as_tensor(input_test)\n",
    "  label_train = torch.as_tensor(label_train)\n",
    "  label_valid = torch.as_tensor(label_valid)\n",
    "  label_test = torch.as_tensor(label_test)\n",
    "\n",
    "  return input_train, input_valid, input_test, label_train, label_valid, label_test\n",
    "\n",
    "def accuracy_calc(predictions, labels):\n",
    "    prediction = torch.argmax(predictions, dim=1)\n",
    "    prediction = nn.functional.one_hot(prediction, num_classes=5)\n",
    "    ret = torch.unique(prediction[prediction == labels], return_counts=True)\n",
    "    if ret[0][-1] == 1: ret = float(ret[1][1]/len(labels))\n",
    "    else: ret = 0.0\n",
    "    return ret\n",
    "\n",
    "# Model Definition\n",
    "class model(nn.Module):\n",
    "    def __init__(self, in_channels=50, out_channels=30, kernel_size=3, stride=1, hidden_dim=4, num_layers=4):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convolution_layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride),      # IN: [batch_size, num_vecs=50, embed_dim=50], OUT: [batch_size, out_channel=30, convolved=48]\n",
    "            nn.BatchNorm1d(out_channels),                                   # unchanged; in-place op\n",
    "            nn.ReLU(),                                                      # unchanged; in-place ops\n",
    "            nn.MaxPool1d(kernel_size))                                      # IN: [batch_size, out_channel=30, convolved=48], OUT: [batch_size, 30, floor(convolved/kernel_size)=16]\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=int((self.in_channels - self.kernel_size + 1)/self.kernel_size), hidden_size=hidden_dim, bidirectional=True)   # IN: [batch_size, 30, 16], OUT: [batch_size, 30, 8]\n",
    "\n",
    "        self.lin1 = nn.Linear(out_channels, 1)\n",
    "        self.lin2 = nn.Linear(8, 5)     # whatever the 2nd of bilstm out is\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.convolution_layer1(x)\n",
    "        x, (hn, cn) = self.bilstm(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.lin2(x)\n",
    "        x = self.softmax(x)\n",
    "        return(x)\n",
    "\n",
    "# training loop\n",
    "def training_loop(model_instance, loss, optimizer, epochs, batch_size, x_train, y_train):\n",
    "  for epoch in range(epochs):\n",
    "    prediction = model_instance(x_train)\n",
    "    print(prediction.shape)\n",
    "    loss_value = loss(prediction, y_train)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in pretrained GloVe embeddings from: D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\n",
      "400000 words loaded into embedding\n"
     ]
    }
   ],
   "source": [
    "runstart = tt.time()\n",
    "small_embeds = load_pretrained_embeddings(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/glove.6B/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished vectorizing csv number 1 Dataset size:  28091 28091\n",
      "Finished vectorizing csv number 2 Dataset size:  65180 65180\n",
      "Finished vectorizing csv number 3 Dataset size:  104505 104505\n",
      "Finished vectorizing csv number 4 Dataset size:  143588 143588\n",
      "Finished vectorizing csv number 5 Dataset size:  207863 207863\n"
     ]
    }
   ],
   "source": [
    "training_set = [[], []]\n",
    "for i in range(1, 6):\n",
    "    train_tokens = tokenize_csv(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/tweets_labels/data{}.csv\".format(str(i)))\n",
    "    temp_vec = tweet_vectorize(train_tokens, small_embeds)\n",
    "    training_set[0] = training_set[0] + temp_vec[0]\n",
    "    training_set[1] = training_set[1] + temp_vec[1]\n",
    "    print('Finished vectorizing csv number {}'.format(str(i)), \"Dataset size: \", len(training_set[0]), len(training_set[1]))\n",
    "\n",
    "manual_names = ['anger', 'fear', 'happy', 'sad']\n",
    "manual_labels = [[1,0,0,0,0], [0,1,0,0,0], [0,0,0,1,0], [0,0,0,0,1]]\n",
    "\n",
    "manual_set = [[], []]\n",
    "for i in range(len(manual_labels)):\n",
    "    manual_tokens = tokenize_manual(\"D:/OneDrive - University of Toronto/School/NSCI Y3/WINTER/ECE324/tweet_emotion/manually_labeled/data_{}.csv\".format(manual_names[i]), manual_labels[i])\n",
    "    temp_vec = tweet_vectorize(manual_tokens, small_embeds)\n",
    "    manual_set[0] = manual_set[0] + temp_vec[0]\n",
    "    manual_set[1] = manual_set[1] + temp_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyx\\AppData\\Local\\Temp/ipykernel_14980/131696540.py:117: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  input_train = torch.as_tensor(input_train)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test = split_train_val_test(training_set[0], training_set[1], manual_set[0], manual_set[1])\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = convert_to_tensors(x_train, x_valid, x_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Tested Parameters\n",
    "Training params: epochs = 10, learning rate = 0.001, batch size = 10000, optimizer = Adam, loss = BCELoss\n",
    "\n",
    "Model params: out_channels = 30, kernel_size = 3, stride = 1, hidden_dim = 4 (hidden_dim - and maybe everything else - affects bilstm out dimensions which need to match lin layers)\n",
    "\n",
    "Train & Valid accuracy w/ params: 60.4906% & 60.3420%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\garyx\\AppData\\Local\\Temp/ipykernel_14980/131696540.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4814\t\t\n",
      "Train Acc: 45.6831%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 1/10 [00:19<02:57, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.4671\t\t\n",
      "Valid Acc: 60.0301%\t\n",
      "Train Loss: 0.4518\t\t\n",
      "Train Acc: 60.0749%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 2/10 [00:37<02:28, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.4326\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.4130\t\t\n",
      "Train Acc: 60.0749%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 3/10 [00:54<02:06, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3952\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.3845\t\t\n",
      "Train Acc: 60.0749%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 4/10 [01:12<01:47, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3765\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.3723\t\t\n",
      "Train Acc: 60.0749%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 5/10 [01:29<01:27, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3697\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.3675\t\t\n",
      "Train Acc: 60.0749%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 6/10 [01:45<01:08, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3661\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.3637\t\t\n",
      "Train Acc: 60.0772%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 7/10 [02:01<00:50, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3622\t\t\n",
      "Valid Acc: 60.0234%\t\n",
      "Train Loss: 0.3597\t\t\n",
      "Train Acc: 60.1054%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 8/10 [02:17<00:33, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3581\t\t\n",
      "Valid Acc: 60.0334%\t\n",
      "Train Loss: 0.3553\t\t\n",
      "Train Acc: 60.2157%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 9/10 [02:34<00:16, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3535\t\t\n",
      "Valid Acc: 60.1201%\t\n",
      "Train Loss: 0.3505\t\t\n",
      "Train Acc: 60.4906%\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 10/10 [02:50<00:00, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.3483\t\t\n",
      "Valid Acc: 60.3420%\t\n",
      "Total training runtime: 170.709361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = tt.time()\n",
    "\n",
    "y_train = torch.squeeze(y_train, dim=1)\n",
    "y_valid = torch.squeeze(y_valid, dim=1)\n",
    "y_test = torch.squeeze(y_test, dim=1)\n",
    "\n",
    "model_instance = model()\n",
    "\n",
    "# Training hyperparams\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr=0.001)\n",
    "batch_size = 10000\n",
    "num_epochs = 10\n",
    "\n",
    "curr_iter = 0\n",
    "for epoch in tqdm(range(int(num_epochs)), desc='Training Epochs'):\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    for b in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[b:b+batch_size]\n",
    "        y_batch = y_train[b:b+batch_size]\n",
    "\n",
    "        prediction = model_instance(x_batch)\n",
    "        prediction = torch.squeeze(prediction, dim=1)\n",
    "        y_batch = y_batch.type(torch.cuda.FloatTensor)\n",
    "        loss_value = loss(prediction, y_batch)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_acc.append(accuracy_calc(prediction, y_batch))\n",
    "        train_loss.append(loss_value.item())\n",
    "        \n",
    "    train_loss = np.asarray(train_loss)\n",
    "    train_acc = np.asarray(train_acc)\n",
    "    print(\"Train Loss: {:.4f}\\t\\t\".format(np.mean(train_loss))) \n",
    "    print(\"Train Acc: {:.4f}%\\t\".format(100*np.mean(train_acc)))\n",
    "\n",
    "    for b in range(0, len(x_valid), batch_size):\n",
    "        x_batch = x_valid[b:b+batch_size]\n",
    "        y_batch = y_valid[b:b+batch_size]\n",
    "\n",
    "        prediction = model_instance(x_batch)\n",
    "        prediction = torch.squeeze(prediction, dim=1)\n",
    "        y_batch = y_batch.type(torch.cuda.FloatTensor)\n",
    "        loss_value = loss(prediction, y_batch)\n",
    "\n",
    "        valid_acc.append(accuracy_calc(prediction, y_batch))\n",
    "        valid_loss.append(loss_value.item())\n",
    "\n",
    "    valid_loss = np.asarray(valid_loss)\n",
    "    valid_acc = np.asarray(valid_acc)\n",
    "    print(\"Valid Loss: {:.4f}\\t\\t\".format(np.mean(valid_loss))) \n",
    "    print(\"Valid Acc: {:.4f}%\\t\".format(100*np.mean(valid_acc)))\n",
    "\n",
    "print(\"Total training runtime: {:2f}\".format(runtime(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyx\\AppData\\Local\\Temp/ipykernel_14980/131696540.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 59.994340\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy:\n",
    "prediction = model_instance(x_test)\n",
    "prediction = torch.squeeze(prediction, dim=1)\n",
    "test_acc = accuracy_calc(prediction, y_test)\n",
    "print(\"Test acc: {:4f}\".format(100*test_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10e1abf77ddef6b5a0d462bd0c1af8a6d69cacf984b7ff54f171d3af9bd75ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
